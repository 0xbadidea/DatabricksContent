{"cells":[{"cell_type":"markdown","source":["Upserting hourly batch bikesharing data\n1. Create an intial gold table that is missing the most recent four hours\n1. Upsert with the entire dataset"],"metadata":{}},{"cell_type":"code","source":["# Read in our data\ndf = spark.read.format('delta').load(\"/mnt/delta/silver/bikeSharing/hourly\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# subtract a few hours from the max date so we can filter\nfrom datetime import timedelta\nmax_datetime = df.agg({\"dteday\": \"max\"}).collect()[0][0]-timedelta(hours=4)\nmax_datetime"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Summarize data, filter to keep away the most recent 2 hours, write the data\nfrom pyspark.sql.functions import col, unix_timestamp\nfrom pyspark.sql.types import DateType\nsummary_df = (df \n              .withColumn(\"datetime\", (unix_timestamp(col(\"dteday\"))+col(\"hr\")*3600).cast(\"timestamp\")) # we create a datetime column to filter on using the hr column and dteday column\n              .filter(col(\"datetime\") < max_datetime)\n              .select(\"dteday\", \"cnt\")\n              .groupBy(\"dteday\")\n              .sum(\"cnt\")\n              .select(col(\"dteday\").alias(\"date\"), col(\"sum(cnt)\").alias(\"cnt\")))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# display, and use the html table to sort and see '2012-12-30'\ndisplay(summary_df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Write the data\nsummary_df.write.format(\"delta\").save(\"/mnt/delta/gold/bikeSharing/daily_summar_for_upsert\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Quickly see how we are able to enforce a schema with Delta!\n\nNotice the following:\n- Line 11: Cast the date column to a date type (was a timestamp)\n- Line 13: We are overwriting the delta table (as opposed to appending)"],"metadata":{}},{"cell_type":"code","source":["#### This should throw an Error\n## See the schema enforcement\n# Summarize data, filter to keep away the most recent 2 hours, write the data\nfrom pyspark.sql.functions import col, unix_timestamp\nfrom pyspark.sql.types import DateType\nsummary_df = (df \n              .withColumn(\"datetime\", (unix_timestamp(col(\"dteday\"))+col(\"hr\")*3600).cast(\"timestamp\"))\n              .filter(col(\"datetime\") < max_datetime)\n              .select(\"dteday\", \"cnt\")\n              .groupBy(\"dteday\")\n              .sum(\"cnt\")\n              .select(col(\"dteday\").alias(\"date\").cast(\"date\"), col(\"sum(cnt)\").alias(\"cnt\")))\n\nsummary_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/gold/bikeSharing/daily_summar_for_upsert\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Lets do an upsert"],"metadata":{}},{"cell_type":"code","source":["# Reread the data, summarize and upsert\nwhole_df = spark.read.format('delta').load(\"/mnt/delta/silver/bikeSharing/hourly\")\n\nupsert_df = (whole_df\n              .withColumn(\"date\", col(\"dteday\").cast(DateType()))\n              .select(\"date\", \"cnt\")\n              .groupBy(\"date\")\n              .sum(\"cnt\")\n              .select(col(\"date\"), col(\"sum(cnt)\").alias(\"cnt\")))\n\n# register as a temp table\nupsert_df.registerTempTable(\"bike_upsert\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# display to show diff, filter to see the '2012-12-31' row is added to the whole dataframe and the '2012-12-30' is a different number\ndisplay(upsert_df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# register delta table in our database. Open the \"Data\" tab and you will see it in the default database. \nspark.sql(\"\"\"\n  DROP TABLE IF EXISTS bike_counts\n\"\"\")\n\nspark.sql(\"\"\"\n  CREATE TABLE bike_counts\n  USING DELTA\n  LOCATION '{}'\n\"\"\".format(\"/mnt/delta/gold/bikeSharing/daily_summar_for_upsert\"))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql\n\nMERGE INTO bike_counts\nUSING bike_upsert\nON bike_counts.date = bike_upsert.date\nWHEN MATCHED THEN\n  UPDATE SET cnt = bike_upsert.cnt\nWHEN NOT MATCHED THEN\n  INSERT *"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Load and display delta table to see that it was updated appropriately\ndisplay(spark.sql(\"select * from bike_counts\"))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Now lets check out the time travel feature! \n\nThe delta table that we just created has two versions available. Lets check out both versions."],"metadata":{}},{"cell_type":"code","source":["%sql \nDESCRIBE HISTORY bike_counts"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\nSELECT *\nFROM bike_counts\nVERSION AS OF 0"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["You can even query and compare between the different versions of the table"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*) - (\n  SELECT count(*)\n  FROM bike_counts\n  VERSION AS OF 0 ) AS new_entries\nFROM bike_counts"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"05_DeltaFeatures","notebookId":570773972606985},"nbformat":4,"nbformat_minor":0}
